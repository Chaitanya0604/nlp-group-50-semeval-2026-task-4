{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Chaitanya0604/nlp-group-50-semeval-2026-task-4/blob/main/Narrative_Similarity_track_a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use Google Drive (persistent)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DATA_PATH = \"/content/drive/MyDrive/semeval-2026-task-4-baselines/data\"\n",
    "OUTPUT_PATH = \"/content/drive/MyDrive/SemEvalProjectNLP/track_a\"\n",
    "\n",
    "# Option 2: Use local content folder (temporary)\n",
    "# DATA_PATH = \"/content/semeval-2026-task-4-baselines/data\"\n",
    "# OUTPUT_PATH = \"/content/track_a\"\n",
    "\n",
    "import os\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)  # create output folder if it doesn't exist\n",
    "print(\"Output folder:\", OUTPUT_PATH)\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpMbDSZIEw_A"
   },
   "source": [
    "Step 1: Setup & Imports\n",
    "\n",
    "We import all the necessary Python libraries for data handling, preprocessing, embedding generation, and modeling.\n",
    "We also set the paths for the dataset and output folders. The notebook can be configured to work either with Google Drive\n",
    "for persistent storage or with the local /content folder in Colab for temporary storage.\n",
    "Creating the output folder ensures that all predictions and results are saved properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(os.path.join(DATA_PATH, \"dev_track_a.jsonl\"), lines=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJ-MbumoGygU"
   },
   "source": [
    "Step 2: Load Dataset\n",
    "\n",
    "We load the JSONL dataset containing triples of (Anchor, Story A, Story B) with a label indicating which story is more similar to the anchor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = \"/content/drive/MyDrive/semeval-2026-task-4-baselines/data\"\n",
    "OUTPUT_PATH = \"/content/drive/MyDrive/semeval-2026-task-4-baselines/output\"\n",
    "\n",
    "import os\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZaDxZzfKgzf"
   },
   "source": [
    "Step 3: Preprocessing\n",
    "\n",
    "We normalize the text: remove extra spaces, lowercase everything, and normalize quotes. Punctuation is preserved for discourse features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize text:\n",
    "    - Strip leading/trailing spaces\n",
    "    - Replace multiple spaces with single space\n",
    "    - Normalize quotes\n",
    "    - Convert to lowercase\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # remove extra spaces\n",
    "    text = text.replace('“', '\"').replace('”', '\"').replace(\"’\", \"'\")\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "for col in [\"anchor_text\", \"text_a\", \"text_b\"]:\n",
    "    df[col] = df[col].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCHJcchGKD5W"
   },
   "source": [
    "Step 4: Convert Triples to Pairwise Examples\n",
    "\n",
    "To train a similarity model, we convert each triple into two pairwise examples:\n",
    "* (Anchor, A) → label = 1 if A is closer, else 0\n",
    "* (Anchor, B) → label = 1 if B is closer, else 0\n",
    "\n",
    "We convert each triple into two pairwise samples for training the Siamese and Cross-Encoder models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_data = []\n",
    "for _, row in df.iterrows():\n",
    "    # Pair: (Anchor, A) → 1 if A is closer else 0\n",
    "    pairwise_data.append({\n",
    "        \"anchor\": row[\"anchor_text\"],\n",
    "        \"candidate\": row[\"text_a\"],\n",
    "        \"label\": 1 if row[\"text_a_is_closer\"] else 0\n",
    "    })\n",
    "    # Pair: (Anchor, B) → 1 if B is closer else 0\n",
    "    pairwise_data.append({\n",
    "        \"anchor\": row[\"anchor_text\"],\n",
    "        \"candidate\": row[\"text_b\"],\n",
    "        \"label\": 1 if not row[\"text_a_is_closer\"] else 0\n",
    "    })\n",
    "\n",
    "pairwise_df = pd.DataFrame(pairwise_data)\n",
    "pairwise_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1_j0t65MIHI"
   },
   "source": [
    "Step 5: Embedding Creation (Hugging Face)\n",
    "\n",
    "We generate embeddings using sentence-transformers/all-mpnet-base-v2. Both anchors and candidates are encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "embed_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Encode all anchors and candidates\n",
    "anchor_embeddings = embed_model.encode(pairwise_df[\"anchor\"].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "candidate_embeddings = embed_model.encode(pairwise_df[\"candidate\"].tolist(), convert_to_tensor=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUndYvcDNvS_"
   },
   "source": [
    "Step 5 – Embedding Creation with Optional Models\n",
    "\n",
    "You can generate embeddings using different types of transformer models depending on your needs:\n",
    "Semantic embeddings: Use google/bert-base for general-purpose sentence embeddings with fine-tuning.\n",
    "\n",
    "Discourse-aware embeddings: Use models like facebook/bart-large or roberta-large fine-tuned on NLI/STS tasks to capture narrative coherence and story-level relations.\n",
    "\n",
    "You can compute embeddings for each story (anchor, A, B) and later compare which model gives better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Embedding Creation with Optional Models\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Choose ANY HuggingFace model:\n",
    "# Semantic: \"bert-base-uncased\"\n",
    "# Discourse-aware: \"facebook/bart-large\", \"roberta-large-mnli\"\n",
    "\n",
    "model_name = \"bert-base-uncased\"  # change this if you want another model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a single text using mean pooling.\n",
    "    Works for semantic or discourse-aware models.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "# APPLY TO YOUR SINGLE DATAFRAME\n",
    "df[\"anchor_emb\"] = df[\"anchor_text\"].apply(get_embedding)\n",
    "df[\"text_a_emb\"] = df[\"text_a\"].apply(get_embedding)\n",
    "df[\"text_b_emb\"] = df[\"text_b\"].apply(get_embedding)\n",
    "\n",
    "print(\"Embeddings computed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RAgw5P-MSIA"
   },
   "source": [
    "\n",
    "Step 6: Siamese Model\n",
    "\n",
    "We build a Siamese network that takes embeddings of (anchor, candidate) and outputs a similarity score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim=768):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim*2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, anchor_emb, candidate_emb):\n",
    "        x = torch.cat([anchor_emb, candidate_emb], dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "siamese_model = SiameseNetwork().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icWZeNpbMizQ"
   },
   "source": [
    "Step 7: Cross-Encoder Model\n",
    "\n",
    "We use a Hugging Face CrossEncoder to predict similarity on concatenated text pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_model = CrossEncoder('cross-encoder/stsb-roberta-large', num_labels=1, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lL5JXU2SMpsE"
   },
   "source": [
    "Step 8: Ensemble Prediction Logic\n",
    "\n",
    "We combine three methods: Siamese similarity score, Cross-Encoder score, and cosine similarity of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ensemble(anchor, text_a, text_b):\n",
    "    \"\"\"\n",
    "    Ensemble prediction using:\n",
    "    - Siamese network\n",
    "    - Cross-encoder\n",
    "    - Cosine similarity\n",
    "    \"\"\"\n",
    "    anchor = preprocess_text(anchor)\n",
    "    text_a = preprocess_text(text_a)\n",
    "    text_b = preprocess_text(text_b)\n",
    "\n",
    "    anchor_emb = embed_model.encode([anchor], convert_to_tensor=True).to(device)\n",
    "    a_emb = embed_model.encode([text_a], convert_to_tensor=True).to(device)\n",
    "    b_emb = embed_model.encode([text_b], convert_to_tensor=True).to(device)\n",
    "\n",
    "    # Siamese\n",
    "    siam_a = siamese_model(anchor_emb, a_emb).item()\n",
    "    siam_b = siamese_model(anchor_emb, b_emb).item()\n",
    "\n",
    "    # Cosine similarity\n",
    "    cos_a = cosine_similarity(anchor_emb.cpu().numpy(), a_emb.cpu().numpy())[0][0]\n",
    "    cos_b = cosine_similarity(anchor_emb.cpu().numpy(), b_emb.cpu().numpy())[0][0]\n",
    "\n",
    "    # Cross-encoder\n",
    "    cross_scores = cross_model.predict([[anchor, text_a], [anchor, text_b]])\n",
    "    cross_a, cross_b = cross_scores[0], cross_scores[1]\n",
    "\n",
    "    # Ensemble average\n",
    "    score_a = np.mean([siam_a, cos_a, cross_a])\n",
    "    score_b = np.mean([siam_b, cos_b, cross_b])\n",
    "\n",
    "    return \"A\" if score_a > score_b else \"B\"\n",
    "\n",
    "# Apply ensemble to dev set\n",
    "df[\"predicted\"] = df.apply(lambda row: predict_ensemble(row[\"anchor_text\"], row[\"text_a\"], row[\"text_b\"]), axis=1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITjdMnpXM6Jg"
   },
   "source": [
    "Step 9: Evaluate\n",
    "\n",
    "We compute accuracy and Pearson correlation between predicted and human labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "df[\"predicted_label\"] = df[\"predicted\"].apply(lambda x: 1 if x==\"A\" else 0)\n",
    "labels = df[\"text_a_is_closer\"].astype(int)\n",
    "\n",
    "accuracy = (df[\"predicted_label\"] == labels).mean()\n",
    "r, _ = pearsonr(df[\"predicted_label\"], labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Pearson r: {r:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Va0p_v0zNH7m"
   },
   "source": [
    "Step 10: Save Output\n",
    "\n",
    "We save predictions in track_a.jsonl in the output folder. This can be in /content or Google Drive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = os.path.join(OUTPUT_PATH, \"track_a.jsonl\")\n",
    "df.to_json(output_file, orient='records', lines=True)\n",
    "print(\"Saved predictions to:\", output_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
